{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeWeZTKxgvi7"
   },
   "source": [
    "# ROME PROJECT\n",
    "###### Daniel Elechiguerra Batlle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ilUoNi5gvi-"
   },
   "source": [
    "I have coded the general parameters so that only the cells that have assignments need to be changed.\n",
    "\n",
    "As for the algorithms, changing the parameters that appear at the beginning of the cell should be enough and it should run automatically without the need to change anything else. You can also optimize the hyperparameters by: **Grid Search, Random Search, Bayesian Optimization y Genetic Algorithms**.\n",
    "\n",
    "Parameters and results for **MLP, GRU, RNN y LSTM (y BI-LSTM)** are automatically logged in **\"Results1.xlsx\"** in order to being able of comparing results. **CNN** usses different hyperparameters and should be saved independently. \n",
    "Currently, I am saving (but this information could be expanded if necessary):\n",
    "\n",
    "\n",
    "> *Algorithm,\tLayers,\tNeurons per layer,\tActivation,\tOptimizer,\tLearning rate, Horizon,\tBack steps,\tBatch size,\tEpochs,\tDataset,\tLearning MSE,\tValidation MSE,\tTraning time*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBPUPZB3gvi_"
   },
   "source": [
    "##### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 2738,
     "status": "ok",
     "timestamp": 1590430418280,
     "user": {
      "displayName": "ROME Project",
      "photoUrl": "",
      "userId": "15433971874005411472"
     },
     "user_tz": -120
    },
    "id": "z-my6C-CgvjA",
    "outputId": "22701375-f698-40b9-c8df-ec49d939843c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import time\n",
    "import re\n",
    "import random as rn\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, GRU, SimpleRNN, Conv1D, Flatten, Dropout, MaxPooling1D, Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRzc0bMTgvjE"
   },
   "source": [
    "##### Select working dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhBgFltTgvjE"
   },
   "outputs": [],
   "source": [
    "dataset = '1h' # Choose 15min (more samples) or 1h (less samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCt1MLwNgvjI"
   },
   "source": [
    "##### Aux Functions (No debería hacer falta modificar nada aquí)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eG9GLoqggvjJ"
   },
   "outputs": [],
   "source": [
    "# This function transforms the input set into 2D/3D inputs\n",
    "# 3D: (samples, back_steps, features)\n",
    "# 2D: (samples, back_steps*features)\n",
    "\n",
    "def create_dataset(old_x, old_y, back_steps = 1, horizon = 1, task = '3d'):\n",
    "    \n",
    "    new_x, new_y = pd.DataFrame(), pd.DataFrame()\n",
    "    for i in range(1, back_steps + 1):\n",
    "        columns = []\n",
    "        for column in list(old_x.columns):\n",
    "            columns.append(column + ' - ' + str(i))\n",
    "        new_x[columns] = old_x.shift(i)\n",
    "        new_x['y - ' + str(i)] = old_y.shift(i)\n",
    "    new_x = new_x[back_steps + horizon - 1:].reset_index(drop = True)\n",
    "    new_y = old_y[back_steps + horizon - 1:].reset_index(drop = True)\n",
    "    if task == '3d':\n",
    "        return np.array(new_x).reshape((np.array(new_x).shape[0], back_steps, int(np.array(new_x).shape[1]/back_steps))), np.array(new_y)\n",
    "    elif task == '2d':\n",
    "        return new_x, new_y\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PktPuCoYgvjM"
   },
   "outputs": [],
   "source": [
    "# Load data (careful with the loading if next function is moved to a different script)\n",
    "\n",
    "data = pd.read_csv('TTOTdataEEM17v3.csv', index_col = 0).reset_index(drop = True)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "if dataset == '15min': \n",
    "    data.drop(['Date', 'Hour', 'Min'], axis = 1, inplace = True)\n",
    "    x = data.iloc[:,:-1]\n",
    "    x = x[x.columns[1::3]]\n",
    "    y = pd.DataFrame(data.iloc[:,-1])\n",
    "else: \n",
    "    data_hour = data.groupby(by = ['Date', 'Hour']).mean()\n",
    "    data_hour = data_hour.reset_index(drop = True)\n",
    "    x = data_hour.iloc[:,:-1]\n",
    "    x = x[x.columns[1::3]]\n",
    "    y = pd.DataFrame(data_hour.iloc[:,-1])\n",
    "    \n",
    "def prepare_data(task = '3d', back_steps = 1, horizon = 1, valid_size = 0.2, shuffle = False):\n",
    "\n",
    "    # Train / Valid\n",
    "    if shuffle:\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(x, y, valid_size = valid_size, random_state = 42)\n",
    "    else:\n",
    "        x_train = x.iloc[:int((1 - valid_size)*len(x)),:]\n",
    "        x_valid = x.iloc[int((1 - valid_size)*len(x)):,:]\n",
    "        y_train = y.iloc[:int((1 - valid_size)*len(y)),:]\n",
    "        y_valid = y.iloc[int((1 - valid_size)*len(y)):,:].reset_index(drop = True)\n",
    "\n",
    "    # Scale input\n",
    "    scaler = StandardScaler()\n",
    "    x_train = pd.DataFrame(scaler.fit_transform(x_train), columns = x_train.columns)\n",
    "    x_valid = pd.DataFrame(scaler.transform(x_valid), columns = x_valid.columns)\n",
    "    \n",
    "    # Scale output\n",
    "    \n",
    "    # Create dataset\n",
    "    if task == '3d':\n",
    "        x_train, y_train = create_dataset(x_train, y_train, back_steps, horizon, '3d')\n",
    "        x_valid, y_valid = create_dataset(x_valid, y_valid, back_steps, horizon, '3d')\n",
    "    elif task == '2d':\n",
    "        x_train, y_train = create_dataset(x_train, y_train, back_steps, horizon, '2d')\n",
    "        x_valid, y_valid = create_dataset(x_valid, y_valid, back_steps, horizon, '2d')\n",
    "    \n",
    "    return x_train, y_train, x_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sI_NEhrDTO5r"
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "def set_seed(seed = seed):\n",
    "    \n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    np.random.seed(seed)\n",
    "    rn.seed(seed)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "    tf.random.set_seed(1234)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNDPLM5ngvjR"
   },
   "source": [
    "###### Select General Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1aTFq2sgvjT"
   },
   "outputs": [],
   "source": [
    "# General Parameters\n",
    "back_steps = 10\n",
    "horizon = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Lqc6MMagvjZ"
   },
   "source": [
    "##### Benchmark MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1133,
     "status": "ok",
     "timestamp": 1590430447846,
     "user": {
      "displayName": "ROME Project",
      "photoUrl": "",
      "userId": "15433971874005411472"
     },
     "user_tz": -120
    },
    "id": "bdUCV3jygvja",
    "outputId": "fb404abe-cbea-4ad3-bbc6-ed6fa6c50d92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark MSE: 272.83736866408935\n"
     ]
    }
   ],
   "source": [
    "# Error of predicting the same output as the last interval value can be used as benchmark\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = prepare_data(task = '2d', back_steps = back_steps, horizon = horizon)\n",
    "mse_benchmark = sklearn.metrics.mean_squared_error(y_valid[horizon:], y_valid[:y_valid.shape[0] - horizon])\n",
    "print('Benchmark MSE:', mse_benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CK2RZnIX9vG2"
   },
   "source": [
    "### Hyperparameter Optimization \n",
    "###### (todas las celdas anteriores son necesarias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQthHJEk95gl"
   },
   "source": [
    "#### Define Models \n",
    "###### (esta celda hay que correrla para todos los tipos de optimización)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4J_epPvGLkVt"
   },
   "source": [
    "##### MLP, GRU, RNN, LSTM, BI LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59Kx1sVZrP7u"
   },
   "outputs": [],
   "source": [
    "def create_model(algorithm, activation, optimizer, learning_rate, layers, neurons_per_layer, shape):\n",
    "\n",
    "    # Layer hyperparameters and number of layers  \n",
    "    neurons_per_layer = [int(neurons_per_layer)]*int(layers)  \n",
    "\n",
    "    # Optimizer\n",
    "    if optimizer == 'adam':\n",
    "      opt = keras.optimizers.Adam(lr = learning_rate)\n",
    "    elif optimizer == 'nadam':\n",
    "      opt = keras.optimizers.Nadam(lr = learning_rate)\n",
    "    elif optimizer == 'adadelta':\n",
    "      opt = keras.optimizers.Adadelta(lr = learning_rate)\n",
    "    elif optimizer == 'adagrad':\n",
    "      opt = keras.optimizers.Adagrad(lr = learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "      opt = keras.optimizers.RMSprop(lr = learning_rate)\n",
    "\n",
    "    ## MODEL\n",
    "    model = Sequential()\n",
    "\n",
    "    if algorithm == 'mlp':\n",
    "      # Add layers\n",
    "      model.add(Dense(neurons_per_layer[0], input_dim = shape[1], activation = activation)) \n",
    "      for neurons in neurons_per_layer[1:]:\n",
    "          model.add(Dense(neurons, activation = activation))\n",
    "      model.add(Dense(1))\n",
    "\n",
    "    elif algorithm == 'rnn':\n",
    "      # Add layers\n",
    "      if len(neurons_per_layer) == 1:\n",
    "          model.add(SimpleRNN(neurons_per_layer[0], input_shape = (shape[1], shape[2]), return_sequences = False, activation = activation))\n",
    "      else:\n",
    "          model.add(SimpleRNN(neurons_per_layer[0], input_shape = (shape[1], shape[2]), return_sequences = True, activation = activation))\n",
    "          for neurons in neurons_per_layer[1:-1]:\n",
    "              model.add(SimpleRNN(neurons, return_sequences = True, activation = activation))\n",
    "          model.add(SimpleRNN(neurons_per_layer[-1], return_sequences = False, activation = activation))\n",
    "      model.add(Dense(1))\n",
    "\n",
    "    elif algorithm == 'gru':\n",
    "      # Add layers\n",
    "      if len(neurons_per_layer) == 1:\n",
    "          model.add(GRU(neurons_per_layer[0], input_shape = (shape[1], shape[2]), return_sequences = False, activation = activation))\n",
    "      else:\n",
    "          model.add(GRU(neurons_per_layer[0], input_shape = (shape[1], shape[2]), return_sequences = True, activation = activation))\n",
    "          for neurons in neurons_per_layer[1:-1]:\n",
    "              model.add(GRU(neurons, return_sequences = True, activation = activation))\n",
    "          model.add(GRU(neurons_per_layer[-1], return_sequences = False, activation = activation))\n",
    "      model.add(Dense(1))\n",
    "\n",
    "    elif algorithm == 'lstm':\n",
    "      # Add layers\n",
    "      if len(neurons_per_layer) == 1:\n",
    "          model.add(LSTM(neurons_per_layer[0], input_shape = (shape[1], shape[2]), return_sequences = False, activation = activation))\n",
    "      else:\n",
    "          model.add(LSTM(neurons_per_layer[0], input_shape = (shape[1], shape[2]), return_sequences = True, activation = activation))\n",
    "          for neurons in neurons_per_layer[1:-1]:\n",
    "              model.add(LSTM(neurons, return_sequences = True, activation = activation))\n",
    "          model.add(LSTM(neurons_per_layer[-1], return_sequences = False, activation = activation))\n",
    "      model.add(Dense(1))\n",
    "\n",
    "    elif algorithm == 'bilstm':\n",
    "      # Add layers\n",
    "      if len(neurons_per_layer) == 1:\n",
    "          model.add(Bidirectional(LSTM(neurons_per_layer[0], input_shape = (shape[1], shape[2]), return_sequences = False, activation = activation)))\n",
    "      else:\n",
    "          model.add(Bidirectional(LSTM(neurons_per_layer[0], input_shape = (shape[1], shape[2]), return_sequences = True, activation = activation)))\n",
    "          for neurons in neurons_per_layer[1:-1]:\n",
    "              model.add(Bidirectional(LSTM(neurons, return_sequences = True, activation = activation)))\n",
    "          model.add(Bidirectional(LSTM(neurons_per_layer[-1], return_sequences = False, activation = activation)))\n",
    "      model.add(Dense(1))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = opt)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alY6szSIzSbE"
   },
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6dujK4-KgTG"
   },
   "source": [
    "##### GRU, RNN, LSTM, BI LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "executionInfo": {
     "elapsed": 356125,
     "status": "ok",
     "timestamp": 1590196007390,
     "user": {
      "displayName": "ROME Project",
      "photoUrl": "",
      "userId": "15433971874005411472"
     },
     "user_tz": -120
    },
    "id": "Hglg4I0l66fd",
    "outputId": "93666515-7b5a-4a6c-90b9-f85ae0cdba9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 8 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MSE:  172.14781082137594\n",
      "Params:  {'activation': 'elu', 'algorithm': 'gru', 'layers': 2, 'learning_rate': 0.0005000000000000001, 'neurons_per_layer': 24, 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Hiperparámetros a optimizar. Se pueden seleccionar uno o varios algoritmos!\n",
    "grid = {'algorithm' : ['gru'], #['gru', 'rnn', 'lstm', 'bilstm'],\n",
    "            'activation' : ['elu'], #['elu', 'relu', 'selu']\n",
    "            'layers' : list(np.arange(1,5,1)), #min_layers, max_layers (no incluido), step_size\n",
    "            'neurons_per_layer' : list(np.arange(16, 28, 4)), #min_neurons, max_neurons (no incluido), step_size\n",
    "            'learning_rate' : list(np.exp(np.linspace(np.log(0.0005),np.log(0.005), 5))), #min_lr, max_lr (incluido), number_of_elements\n",
    "            'optimizer' : ['adam'], #['adam', 'nadam', 'adadelta', 'adagrad', 'rmsprop']\n",
    "            'batch_size' : [20],\n",
    "            'epochs' : [20]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = prepare_data(task = '3d', back_steps = back_steps, horizon = horizon)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn = create_model, shape = x_train.shape, verbose = 0)\n",
    "\n",
    "grid_model = GridSearchCV(estimator = model, param_grid = grid, verbose = 1, refit = False, n_jobs = -1, pre_dispatch = '2*n_jobs', scoring = 'neg_mean_squared_error',  return_train_score = True, \n",
    "                          cv = [(list(np.arange(0,x_train.shape[0],1)),list(np.arange(x_train.shape[0], x_train.shape[0]+x_valid.shape[0],1)))])\n",
    "grid_result = grid_model.fit(np.concatenate((x_train, x_valid)),np.concatenate((y_train, y_valid)))\n",
    "\n",
    "# save results\n",
    "results = pd.read_excel('Results1.xlsx')\n",
    "for i in range(0,len(grid_model.cv_results_['mean_test_score'])):\n",
    "  results.loc[len(results)] = [grid_model.cv_results_['params'][i]['algorithm'], int(grid_model.cv_results_['params'][i]['layers']), int(grid_model.cv_results_['params'][i]['neurons_per_layer']), \n",
    "                              grid_model.cv_results_['params'][i]['activation'], grid_model.cv_results_['params'][i]['optimizer'], grid_model.cv_results_['params'][i]['learning_rate'], \n",
    "                              horizon, back_steps, grid_model.cv_results_['params'][i]['batch_size'],grid_model.cv_results_['params'][i]['epochs'], dataset, -grid_model.cv_results_['mean_train_score'][i], -grid_model.cv_results_['mean_test_score'][i], grid_model.cv_results_['mean_fit_time'][i]]\n",
    "results.to_excel('Results1.xlsx', index = False)\n",
    "\n",
    "print('Best MSE: ', -grid_model.best_score_)\n",
    "print('Params: ', grid_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3uI9NiLclrz"
   },
   "source": [
    "##### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "executionInfo": {
     "elapsed": 138464,
     "status": "ok",
     "timestamp": 1590196146369,
     "user": {
      "displayName": "ROME Project",
      "photoUrl": "",
      "userId": "15433971874005411472"
     },
     "user_tz": -120
    },
    "id": "G0K3OqgMVASd",
    "outputId": "6b6b626b-4797-437a-ae00-99bd3f6539dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 24 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MSE:  235.1434952721103\n",
      "Params:  {'activation': 'elu', 'algorithm': 'mlp', 'layers': 2, 'learning_rate': 0.005, 'neurons_per_layer': 16, 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Hiperparámetros a optimizar. Se pueden seleccionar uno o varios algoritmos!\n",
    "grid = {'algorithm' : ['mlp'],\n",
    "            'activation' : ['elu'], #['elu', 'relu', 'selu', ]\n",
    "            'layers' : list(np.arange(1,5,1)), #min_layers, max_layers (no incluido), step_size\n",
    "            'neurons_per_layer' : list(np.arange(16, 28, 4)), #min_neurons, max_neurons (no incluido), step_size\n",
    "            'learning_rate' : list(np.exp(np.linspace(np.log(0.0005),np.log(0.005), 5))), #min_lr, max_lr (incluido), number_of_elements\n",
    "            'optimizer' : ['adam'], #['adam', 'nadam', 'adadelta', 'adagrad', 'rmsprop']\n",
    "            'batch_size' : [20],\n",
    "            'epochs' : [20]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = prepare_data(task = '2d', back_steps = back_steps, horizon = horizon)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn = create_model, shape = x_train.shape, verbose = 0)\n",
    "\n",
    "grid_model = GridSearchCV(estimator = model, param_grid = grid, verbose = 1, refit = False, n_jobs = -1, pre_dispatch = '2*n_jobs', scoring = 'neg_mean_squared_error', return_train_score = True,\n",
    "                          cv = [(list(np.arange(0,x_train.shape[0],1)),list(np.arange(x_train.shape[0], x_train.shape[0]+x_valid.shape[0],1)))])\n",
    "grid_result = grid_model.fit(np.concatenate((x_train, x_valid)),np.concatenate((y_train, y_valid)))\n",
    "\n",
    "# save results\n",
    "results = pd.read_excel('Results1.xlsx')\n",
    "for i in range(0,len(grid_model.cv_results_['mean_test_score'])):\n",
    "  results.loc[len(results)] = [grid_model.cv_results_['params'][i]['algorithm'], int(grid_model.cv_results_['params'][i]['layers']), int(grid_model.cv_results_['params'][i]['neurons_per_layer']), \n",
    "                              grid_model.cv_results_['params'][i]['activation'], grid_model.cv_results_['params'][i]['optimizer'], grid_model.cv_results_['params'][i]['learning_rate'], \n",
    "                              horizon, back_steps, grid_model.cv_results_['params'][i]['batch_size'],grid_model.cv_results_['params'][i]['epochs'], dataset, -grid_model.cv_results_['mean_train_score'][i], -grid_model.cv_results_['mean_test_score'][i], grid_model.cv_results_['mean_fit_time'][i]]\n",
    "results.to_excel('Results1.xlsx', index = False)\n",
    "\n",
    "print('Best MSE: ', -grid_model.best_score_)\n",
    "print('Params: ', grid_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4NNzp5VDRLF"
   },
   "source": [
    "#### Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOkl6CAiDars"
   },
   "source": [
    "##### GRU, RNN, LSTM, BI LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "executionInfo": {
     "elapsed": 326771,
     "status": "ok",
     "timestamp": 1590197382424,
     "user": {
      "displayName": "ROME Project",
      "photoUrl": "",
      "userId": "15433971874005411472"
     },
     "user_tz": -120
    },
    "id": "aW8UgkCIDag-",
    "outputId": "307ccdfc-770e-497b-b21c-e5e81306c82a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 4 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MSE:  186.2228491198188\n",
      "Params:  {'optimizer': 'adam', 'neurons_per_layer': 22, 'learning_rate': 0.0020474575311902133, 'layers': 1, 'algorithm': 'gru', 'activation': 'elu'}\n"
     ]
    }
   ],
   "source": [
    "iterations = 20\n",
    "\n",
    "# Hiperparámetros a optimizar. Se pueden seleccionar uno o varios algoritmos!\n",
    "grid = {'algorithm' : ['gru'], #['gru', 'rnn', 'lstm', 'bilstm'],\n",
    "            'activation' : ['elu'], #['elu', 'relu', 'selu', ]\n",
    "            'layers' : list(np.arange(1,6,1)), #min_layers, max_layers (no incluido), step_size\n",
    "            'neurons_per_layer' : list(np.arange(16, 34, 2)), #min_neurons, max_neurons (no incluido), step_size\n",
    "            'learning_rate' : list(np.exp(np.linspace(np.log(0.0005),np.log(0.005), 50))), #min_lr, max_lr (incluido), number_of_elements\n",
    "            'optimizer' : ['adam'], #['adam', 'nadam', 'adadelta', 'adagrad', 'rmsprop']\n",
    "            'batch_size' : [20],\n",
    "            'epochs' : [20]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = prepare_data(task = '3d', back_steps = back_steps, horizon = horizon)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn = create_model, shape = x_train.shape, verbose = 0)\n",
    "\n",
    "grid_model = RandomizedSearchCV(estimator = model, param_distributions = grid, n_iter = iterations, verbose = 1, refit = False, n_jobs = -1, pre_dispatch = '2*n_jobs', scoring = 'neg_mean_squared_error',  \n",
    "                                return_train_score = True, cv = [(list(np.arange(0,x_train.shape[0],1)),list(np.arange(x_train.shape[0], x_train.shape[0]+x_valid.shape[0],1)))])\n",
    "grid_result = grid_model.fit(np.concatenate((x_train, x_valid)),np.concatenate((y_train, y_valid)))\n",
    "\n",
    "# save results\n",
    "results = pd.read_excel('Results1.xlsx')\n",
    "for i in range(0,len(grid_model.cv_results_['mean_test_score'])):\n",
    "  results.loc[len(results)] = [grid_model.cv_results_['params'][i]['algorithm'], int(grid_model.cv_results_['params'][i]['layers']), int(grid_model.cv_results_['params'][i]['neurons_per_layer']), \n",
    "                              grid_model.cv_results_['params'][i]['activation'], grid_model.cv_results_['params'][i]['optimizer'], grid_model.cv_results_['params'][i]['learning_rate'], \n",
    "                              horizon, back_steps, grid_model.cv_results_['params'][i]['batch_size'],grid_model.cv_results_['params'][i]['epochs'], dataset, -grid_model.cv_results_['mean_train_score'][i], -grid_model.cv_results_['mean_test_score'][i], grid_model.cv_results_['mean_fit_time'][i]]\n",
    "results.to_excel('Results1.xlsx', index = False)\n",
    "\n",
    "print('Best MSE: ', -grid_model.best_score_)\n",
    "print('Params: ', grid_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gU2dyYChDXyI"
   },
   "source": [
    "##### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "executionInfo": {
     "elapsed": 42729,
     "status": "ok",
     "timestamp": 1590353431648,
     "user": {
      "displayName": "ROME Project",
      "photoUrl": "",
      "userId": "15433971874005411472"
     },
     "user_tz": -120
    },
    "id": "XaT--qwsDbNL",
    "outputId": "d1d98259-fb8d-4527-80f0-9e69f735b902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   40.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MSE:  233.19771857050605\n",
      "Params:  {'optimizer': 'adam', 'neurons_per_layer': 32, 'learning_rate': 0.0021459671300643893, 'layers': 2, 'epochs': 20, 'batch_size': 20, 'algorithm': 'mlp', 'activation': 'elu'}\n"
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "\n",
    "# Hiperparámetros a optimizar. Se pueden seleccionar uno o varios algoritmos!\n",
    "grid = {'algorithm' : ['mlp'],\n",
    "            'activation' : ['elu'], #['elu', 'relu', 'selu', ]\n",
    "            'layers' : list(np.arange(1,6,1)), #min_layers, max_layers (no incluido), step_size\n",
    "            'neurons_per_layer' : list(np.arange(16, 34, 2)), #min_neurons, max_neurons (no incluido), step_size\n",
    "            'learning_rate' : list(np.exp(np.linspace(np.log(0.0005),np.log(0.005), 50))), #min_lr, max_lr (incluido), number_of_elements\n",
    "            'optimizer' : ['adam'], #['adam', 'nadam', 'adadelta', 'adagrad', 'rmsprop']\n",
    "            'batch_size' : [20],\n",
    "            'epochs' : [20]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = prepare_data(task = '2d', back_steps = back_steps, horizon = horizon)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn = create_model, shape = x_train.shape, verbose = 0)\n",
    "\n",
    "grid_model = RandomizedSearchCV(estimator = model, param_distributions = grid, n_iter = iterations, verbose = 1, refit = False, n_jobs = -1, pre_dispatch = '2*n_jobs', scoring = 'neg_mean_squared_error', \n",
    "                                return_train_score = True, cv = [(list(np.arange(0,x_train.shape[0],1)),list(np.arange(x_train.shape[0], x_train.shape[0]+x_valid.shape[0],1)))])\n",
    "grid_result = grid_model.fit(np.concatenate((x_train, x_valid)),np.concatenate((y_train, y_valid)))\n",
    "\n",
    "# save results\n",
    "results = pd.read_excel('Results1.xlsx')\n",
    "for i in range(0,len(grid_model.cv_results_['mean_test_score'])):\n",
    "  results.loc[len(results)] = [grid_model.cv_results_['params'][i]['algorithm'], int(grid_model.cv_results_['params'][i]['layers']), int(grid_model.cv_results_['params'][i]['neurons_per_layer']), \n",
    "                              grid_model.cv_results_['params'][i]['activation'], grid_model.cv_results_['params'][i]['optimizer'], grid_model.cv_results_['params'][i]['learning_rate'], \n",
    "                              horizon, back_steps, grid_model.cv_results_['params'][i]['batch_size'],grid_model.cv_results_['params'][i]['epochs'], dataset, -grid_model.cv_results_['mean_train_score'][i], -grid_model.cv_results_['mean_test_score'][i], grid_model.cv_results_['mean_fit_time'][i]]\n",
    "results.to_excel('Results1.xlsx', index = False)\n",
    "\n",
    "print('Best MSE: ', -grid_model.best_score_)\n",
    "print('Params: ', grid_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICcuPXc5gzU6"
   },
   "source": [
    "#### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJxk5MEDKn7I"
   },
   "source": [
    "##### MLP, GRU, RNN, LSTM, BI LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "executionInfo": {
     "elapsed": 68044,
     "status": "ok",
     "timestamp": 1590431602830,
     "user": {
      "displayName": "ROME Project",
      "photoUrl": "",
      "userId": "15433971874005411472"
     },
     "user_tz": -120
    },
    "id": "XTpc5pxaDcOt",
    "outputId": "aba516b4-e5d3-4e75-8df4-ce59235d1ccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:07<00:00, 13.48s/it, best loss: 210.89283989600403]\n",
      "\n",
      " {'activation': 1, 'algorithm': 2, 'batch_size': 0, 'epochs': 0, 'layers': 2.0, 'learning_rate': 0.0028969581951935625, 'neurons_per_layer': 16.0, 'optimizer': 0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, space_eval, Trials\n",
    "\n",
    "max_evals = 100\n",
    "\n",
    "# Hiperparámetros a optimizar. Se pueden seleccionar uno o varios algoritmos!\n",
    "space = {'algorithm' : hp.choice('algorithm', ['mlp', 'gru', 'rnn', 'lstm', 'bilstm']), #['mlp', 'gru', 'rnn', 'lstm', 'bilstm'],\n",
    "            'activation' : hp.choice('activation',['elu', 'selu']), #['elu', 'relu', 'selu', ]\n",
    "            'layers' : hp.quniform('layers', 1, 5, 1), #min_layers, max_layers, step_size\n",
    "            'neurons_per_layer' : hp.quniform('neurons_per_layer', 16, 64, 2), #min_neurons, max_neurons, step_size\n",
    "            'learning_rate' : hp.loguniform('learning_rate', np.log(0.00005), np.log(0.005)), #min_lr, max_lr\n",
    "            'optimizer' : hp.choice('optimizer',['adam']), #['adam', 'nadam', 'adadelta', 'adagrad', 'rmsprop']\n",
    "            'batch_size' : hp.choice('batch_size',[20]),\n",
    "            'epochs' : hp.choice('epochs',[20])}\n",
    "         \n",
    "# Se puede incluir en las iteraciones el dropout\n",
    "\n",
    "def objective(params):\n",
    "    try:\n",
    "      set_seed()\n",
    "\n",
    "      # Prepare data\n",
    "      if params['algorithm'] == 'mlp':\n",
    "        x_train, y_train, x_valid, y_valid = prepare_data(task = '2d', back_steps = back_steps, horizon = horizon)\n",
    "      else:\n",
    "        x_train, y_train, x_valid, y_valid = prepare_data(task = '3d', back_steps = back_steps, horizon = horizon)\n",
    "\n",
    "      model = create_model(algorithm = params['algorithm'], layers = int(params['layers']), neurons_per_layer = int(params['neurons_per_layer']),\n",
    "                           activation = params['activation'], optimizer = params['optimizer'], learning_rate = params['learning_rate'], shape = x_train.shape)\n",
    "\n",
    "      # Fit model on the dataset\n",
    "      start_time = time.time()\n",
    "      history = model.fit(x_train, y_train, epochs = int(params['epochs']), batch_size = int(params['batch_size']), validation_data = None, verbose = 0, shuffle = False)\n",
    "      training_time = time.time() - start_time\n",
    "\n",
    "      # Evaluate model\n",
    "      y_pred = model.predict(x_valid)\n",
    "      mse = sklearn.metrics.mean_squared_error(y_valid, y_pred)\n",
    "\n",
    "      results = pd.read_excel('Results.xlsx')\n",
    "      results.loc[len(results)] = [params['algorithm'], int(params['layers']), int(params['neurons_per_layer']), params['activation'], \n",
    "                                   params['optimizer'], params['learning_rate'], horizon, back_steps, int(params['batch_size']), int(params['epochs']), dataset, history.history['loss'][-1], mse, training_time]\n",
    "      results.to_excel('Results.xlsx', index = False)\n",
    "\n",
    "    except:\n",
    "      mse = 1000000.0     \n",
    "      results = pd.read_excel('Results.xlsx')\n",
    "      results.loc[len(results)] = [params['algorithm'], int(params['layers']), int(params['neurons_per_layer']), params['activation'], \n",
    "                                   params['optimizer'], params['learning_rate'], horizon, back_steps, int(params['batch_size']), int(params['epochs']), dataset, 'error', 'error', 'error']\n",
    "      results.to_excel('Results.xlsx', index = False)\n",
    "\n",
    "    return mse     #Minimize\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = objective, space = space, max_evals = max_evals, algo = tpe.suggest, trials = trials)\n",
    "print('\\n', best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HOPn4giRxn8"
   },
   "source": [
    "#### Genetic Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gfif_jLR8xq"
   },
   "source": [
    "##### MLP, GRU, RNN, LSTM, BI LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YN5BbffR0He"
   },
   "outputs": [],
   "source": [
    "def fit_model(algorithm, activation, optimizer, learning_rate, layers, neurons_per_layer, batch_size, epochs):\n",
    "\n",
    "    set_seed()\n",
    "\n",
    "    if algorithm == 'mlp':\n",
    "      x_train, y_train, x_valid, y_valid = prepare_data(task = '2d', back_steps = back_steps, horizon = horizon)\n",
    "    else:\n",
    "      x_train, y_train, x_valid, y_valid = prepare_data(task = '3d', back_steps = back_steps, horizon = horizon)\n",
    "    \n",
    "    model = create_model(algorithm = algorithm, activation = activation, optimizer = optimizer, learning_rate = learning_rate, \n",
    "                         layers = layers, neurons_per_layer = neurons_per_layer, shape = x_train.shape)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train, epochs = epochs, batch_size = batch_size, validation_data = None, verbose = 0, shuffle = False)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return model, history.history['loss'][-1], training_time\n",
    "\n",
    "\n",
    "class Network():\n",
    "    def __init__(self):\n",
    "        #self._epochs = np.random.randint(10, 20)\n",
    "        #self._batch_size = np.random.randint(10, 25)\n",
    "        self._epochs = epochs\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        self._algorithm = rn.choice(['mlp']) #['mlp', 'gru', 'rnn', 'lstm', 'bilstm']\n",
    "\n",
    "        self._layers = np.random.randint(1, 6)\n",
    "        self._neurons_per_layer = 10 + 2 * np.random.randint(1, 28)\n",
    "\n",
    "        self._activation = rn.choice(['elu']) #['relu', 'elu', 'selu']\n",
    "        self._optimizer = rn.choice(['adam']) #['adam', 'nadam', 'adadelta', 'adagrad', 'rmsprop']\n",
    "        self._learning_rate = np.exp(np.log(0.0005) + np.random.rand() * (np.log(0.005) - np.log(0.0005)))\n",
    "\n",
    "        self._mse = 1000000\n",
    "\n",
    "    def init_hyperparams(self):\n",
    "        hyperparams = {\n",
    "            'epochs': self._epochs,\n",
    "            'batch_size': self._batch_size,\n",
    "            'algorithm': self._algorithm,\n",
    "            'layers': self._layers,\n",
    "            'neurons_per_layer': self._neurons_per_layer,\n",
    "            'activation': self._activation,\n",
    "            'optimizer': self._optimizer,\n",
    "            'learning_rate': self._learning_rate\n",
    "        }\n",
    "        return hyperparams\n",
    "\n",
    "def init_networks(population):\n",
    "    return [Network() for _ in range(population)]\n",
    "\n",
    "def fitness_network(network):\n",
    "\n",
    "    hyperparams = network.init_hyperparams()\n",
    "    epochs = hyperparams['epochs']\n",
    "    batch_size = hyperparams['batch_size']\n",
    "    algorithm = hyperparams['algorithm']\n",
    "    layers = hyperparams['layers']\n",
    "    neurons_per_layer = hyperparams['neurons_per_layer']\n",
    "    activation = hyperparams['activation']\n",
    "    optimizer = hyperparams['optimizer']\n",
    "    learning_rate = hyperparams['learning_rate']\n",
    "    \n",
    "    try:\n",
    "        model, training_error, training_time = fit_model(algorithm = algorithm, activation = activation, optimizer = optimizer, learning_rate = learning_rate,\n",
    "                                                          layers = layers, neurons_per_layer = neurons_per_layer, batch_size = batch_size, epochs = epochs)\n",
    "        \n",
    "        if algorithm == 'mlp':\n",
    "          x_train, y_train, x_valid, y_valid = prepare_data(task = '2d', back_steps = back_steps, horizon = horizon)\n",
    "        else:\n",
    "          x_train, y_train, x_valid, y_valid = prepare_data(task = '3d', back_steps = back_steps, horizon = horizon)\n",
    "\n",
    "        y_pred = model.predict(x_valid)\n",
    "        mse = sklearn.metrics.mean_squared_error(y_valid, y_pred)\n",
    "\n",
    "        results = pd.read_excel('Results1.xlsx')\n",
    "        results.loc[len(results)] = [algorithm, int(layers), int(neurons_per_layer), activation, optimizer, learning_rate, horizon, back_steps, batch_size, epochs, dataset,\n",
    "                                      training_error, mse, training_time]\n",
    "        results.to_excel('Results1.xlsx', index = False)\n",
    "\n",
    "        network._mse = mse\n",
    "        print ('MSE: {}'.format(network._mse))\n",
    "\n",
    "    except:\n",
    "        network._mse = 1000000\n",
    "        results = pd.read_excel('Results1.xlsx')\n",
    "        results.loc[len(results)] = [algorithm, int(layers), int(neurons_per_layer), activation, optimizer, learning_rate, horizon, back_steps, batch_size, epochs, dataset,\n",
    "                                      'error', 'error', 'error']\n",
    "        results.to_excel('Results1.xlsx', index = False)\n",
    "        print ('Error')\n",
    "\n",
    "    return network\n",
    "\n",
    "def fitness(networks, parallelization = False):\n",
    "    if parallelization:\n",
    "      n_networks = Parallel(n_jobs=-1)(delayed(fitness_network)(network) for network in networks)\n",
    "      for network in n_networks:\n",
    "        print ('MSE: {}'.format(network._mse))\n",
    "    else:\n",
    "      n_networks = []\n",
    "      for network in networks:\n",
    "        n_networks.append(fitness_network(network))\n",
    "    return n_networks\n",
    "\n",
    "def selection(networks):\n",
    "    networks = sorted(networks, key=lambda network: network._mse, reverse=False)\n",
    "    networks = networks[:max(int(0.2 * len(networks)), 2)]\n",
    "\n",
    "    return networks\n",
    "\n",
    "def crossover(networks):\n",
    "    offspring = []\n",
    "    for _ in range(int((population - len(networks)) / 2)):\n",
    "        parent1 = rn.choice(networks)\n",
    "        parent2 = rn.choice(networks)\n",
    "        child1 = Network()\n",
    "        child2 = Network()\n",
    "\n",
    "        # Crossing over parent hyper-params\n",
    "        child1._epochs = parent2._epochs #max(int(3*parent1._epochs/4 + parent2._epochs/4),10)\n",
    "        child2._epochs = parent1._epochs #max(int(parent1._epochs/4 + 3*parent2._epochs/4),10)\n",
    "\n",
    "        child1._batch_size = parent2._batch_size #max(int(parent1._batch_size/4) + 3*parent2._batch_size/4),10)\n",
    "        child2._batch_size = parent1._batch_size #max(int(3*parent1._batch_size/4) + parent2._batch_size/4),10)\n",
    "\n",
    "        child1._algorithm = parent2._algorithm\n",
    "        child2._algorithm = parent1._algorithm        \n",
    "        \n",
    "        child1._layers = rn.choice([parent1._layers, parent2._layers]) #max(int(3*parent1._layers/4 + parent2._layers/4),1)\n",
    "        child2._layers = rn.choice([parent1._layers, parent2._layers]) #max(int(parent1._layers/4 + 3*parent2._layers/4),1)\n",
    "\n",
    "        child1._neurons_per_layer = max(int(parent1._neurons_per_layer/4 + 3*parent2._neurons_per_layer/4), 8)\n",
    "        child2._neurons_per_layer = max(int(3*parent1._neurons_per_layer/4 + parent2._neurons_per_layer/4), 8)\n",
    "\n",
    "        child1._activation = parent2._activation\n",
    "        child2._activation = parent1._activation\n",
    "\n",
    "        child1._optimizer = parent1._optimizer\n",
    "        child2._optimizer = parent2._optimizer\n",
    "\n",
    "        child1._learning_rate = parent1._learning_rate\n",
    "        child2._learning_rate = parent2._learning_rate\n",
    "\n",
    "        offspring.append(child1)\n",
    "        offspring.append(child2)\n",
    "\n",
    "    networks.extend(offspring)\n",
    "\n",
    "    return networks\n",
    "\n",
    "def mutate(networks):\n",
    "    for network in networks:\n",
    "        if np.random.uniform(0, 1) <= 0.1:\n",
    "            #network._epochs += 5 * np.random.randint(0,3)\n",
    "            #network._batch_size += 5 * np.random.randint(0,3)\n",
    "            network._layers += rn.choice([0,1,1,2])\n",
    "            network._neurons_per_layer += 2 * np.random.randint(0,9)\n",
    "            network._learning_rate *= (0.8 + 0.4 * np.random.rand())\n",
    "\n",
    "    return networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 70940,
     "status": "ok",
     "timestamp": 1590260577025,
     "user": {
      "displayName": "ROME Project",
      "photoUrl": "",
      "userId": "15433971874005411472"
     },
     "user_tz": -120
    },
    "id": "uH8HXaPIR6jb",
    "outputId": "fb8a535f-2475-4648-ba11-f2fe1305994b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1\n",
      "MSE: 332.81160578595086\n",
      "MSE: 270.8185266855574\n",
      "MSE: 293.27991452925016\n",
      "MSE: 322.0624300076036\n",
      "\n",
      "Generation 2\n",
      "MSE: 255.86785873472246\n",
      "MSE: 262.83359241866077\n",
      "MSE: 248.50972829628066\n",
      "MSE: 294.54793611585416\n",
      "\n",
      "{'epochs': 20, 'batch_size': 20, 'algorithm': 'mlp', 'layers': 6, 'neurons_per_layer': 34, 'activation': 'elu', 'optimizer': 'adam', 'learning_rate': 0.0006402269437271258}\n",
      "Best MSE: 248.50972829628066\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "epochs = 20\n",
    "threshold = 170\n",
    "generations = 4\n",
    "population = 20\n",
    "parallelization = True #Colab solo tiene 2 cores\n",
    "\n",
    "networks = init_networks(population)\n",
    "best_mse = np.inf\n",
    "best_network = None\n",
    "\n",
    "for gen in range(generations):\n",
    "    print ('\\nGeneration {}'.format(gen+1))\n",
    "    networks = fitness(networks, parallelization)\n",
    "    networks = selection(networks)\n",
    "    networks = crossover(networks)\n",
    "    networks = mutate(networks)\n",
    "\n",
    "    for network in networks:\n",
    "        if network._mse < best_mse:\n",
    "          best_network = network\n",
    "          best_mse = network._mse \n",
    "\n",
    "    if best_mse < threshold:\n",
    "        print ('\\nThreshold met')\n",
    "        break\n",
    "\n",
    "   \n",
    "print ('\\n' + str(best_network.init_hyperparams()))\n",
    "print ('Best MSE: {}'.format(best_network._mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1501,
     "status": "ok",
     "timestamp": 1590260178973,
     "user": {
      "displayName": "ROME Project",
      "photoUrl": "",
      "userId": "15433971874005411472"
     },
     "user_tz": -120
    },
    "id": "1ITfOaHH2nyI",
    "outputId": "7d0ec9fc-5a48-4c66-cafa-f10147a812fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "networks[3]._mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJbge-ul3DxN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TQthHJEk95gl",
    "4J_epPvGLkVt",
    "alY6szSIzSbE",
    "x6dujK4-KgTG",
    "K3uI9NiLclrz",
    "f4NNzp5VDRLF",
    "eOkl6CAiDars",
    "gU2dyYChDXyI",
    "ICcuPXc5gzU6",
    "ZJxk5MEDKn7I",
    "5HOPn4giRxn8",
    "4gfif_jLR8xq"
   ],
   "name": "Hyperparameter optimization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
